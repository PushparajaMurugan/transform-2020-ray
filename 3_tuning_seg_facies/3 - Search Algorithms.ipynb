{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Algorithms\n",
    "\n",
    "So far we have been using Ray's basic/default Variant Generator in order to perform either:\n",
    "\n",
    " - Grid Search\n",
    " - Random Search\n",
    " \n",
    "However, Raytune has an expanding suite of options available for different [Search Algortihms](https://docs.ray.io/en/latest/tune-searchalg.html):\n",
    "\n",
    " - BayesOpt\n",
    " - HyperOpt\n",
    " - SigOpt\n",
    " - Nevergrad\n",
    " - Scikit-Optimize\n",
    " - Ax\n",
    " - BOHB\n",
    " \n",
    "These bolt in functionality from other well known libraries and make them avilable to us in Raytune. \n",
    "\n",
    "Depending on the library that we use there are some customisations needed to use it but these are not in our training code, but centered around how we define the `search space` and call `ray.run`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch to Bayesian Optimisation\n",
    "\n",
    "Ray's BayesOpt uses the [bayesian-optimisation](https://github.com/fmfn/BayesianOptimization) package which is already installed in our conda environment\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dependencies import *\n",
    "from seg_setup_code import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than loading bayesopt directly we pull in the tune search algorithm tha wraps it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.suggest.bayesopt import BayesOptSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have copied across the same training function from earlier notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from scipy.signal import medfilt\n",
    "from filelock import FileLock\n",
    "\n",
    "def e2e_train_and_test(config, **kwargs):\n",
    "    \n",
    "    # threadsafe\n",
    "#     with FileLock(\"./data.lock\"):\n",
    "    X, y, groups, X_test, y_test, group_test, well_names = setup(kwargs['filepath'])\n",
    "    \n",
    "    # chose your CV strategy\n",
    "    splitter = LeavePGroupsOut(1)\n",
    "    \n",
    "    # run k fold training and validation\n",
    "    f1_scores = [] # keep hold of all individual scores\n",
    "    for train_ind, val_ind in splitter.split(X, y, groups=groups):\n",
    "        pipeline = make_pipeline(RobustScaler(),\n",
    "                                  XGBClassifier())\n",
    "\n",
    "        pipeline.set_params(**config)\n",
    "        pipeline.fit(X[train_ind], y[train_ind])\n",
    "        \n",
    "        y_pred = pipeline.predict(X[val_ind])\n",
    "        \n",
    "        f1_scores.append(f1_score(y_pred, y[val_ind], average='micro'))\n",
    "    \n",
    "        # Clean isolated facies for each well\n",
    "        y_pred = medfilt(y_pred, kernel_size=5)\n",
    "    \n",
    "    # use tunes reporter\n",
    "    tune.track.log(mean_f1_score=np.array(f1_scores).mean(),\n",
    "                std_f1_score=np.array(f1_scores).std(),\n",
    "                # and we can actually add any metrics we like\n",
    "                done=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Search Space\n",
    "\n",
    "In previous examples we have creates the search space for our tuning job using Raytune's distribution functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_tuning_config = {\n",
    "    'xgbclassifier__learning_rate': tune.loguniform(0.001, 0.5),\n",
    "    'xgbclassifier__max_depth': tune.randint(1, 10),\n",
    "    'xgbclassifier__min_child_weight': tune.loguniform(0.1, 10),\n",
    "    'xgbclassifier__n_estimators': tune.randint(5,200),\n",
    "    'xgbclassifier__colsample_bytree': tune.choice([0.4, 0.6, 0.8, 1.0]),\n",
    "    'xgbclassifier__lambda': tune.choice([0,1]),\n",
    "    'xgbclassifier__seed': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when using BayesOpt we need to change this and specify:\n",
    " - the bounds of the parameter space `pbounds`\n",
    " - the form / parameters of a UtilityFunction\n",
    " - a modified config object\n",
    " \n",
    "To start with we convert our tuning config to align with pbounds, which uses simple tuples and only suppports *continuous* spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    \"xgbclassifier__learning_rate\": (0.001, 0.5),\n",
    "    \"xgbclassifier__max_depth\": (1, 10),                # Needs to be discrete!\n",
    "    \"xgbclassifier__min_child_weight\": (0.1, 100),\n",
    "    \"xgbclassifier__n_estimators\": (5, 200),            # Needs to be discrete!\n",
    "    \"xgbclassifier__colsample_bytree\": (0.4, 1.0),\n",
    "    \"xgbclassifier__lambda\": (0, 1)                     # Needs to be true/false 1/0\n",
    "}\n",
    "\n",
    "def parse_config(config):\n",
    "    config[\"xgbclassifier__max_depth\"] = int(round(config[\"xgbclassifier__max_depth\"]))\n",
    "    config[\"xgbclassifier__n_estimators\"] = int(round(config[\"xgbclassifier__n_estimators\"]))\n",
    "    config[\"xgbclassifier__lambda\"] = int(round(config[\"xgbclassifier__lambda\"]))\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_fn_kwargs={\n",
    "    \"kind\": \"ucb\",\n",
    "    \"kappa\": 2.5,\n",
    "    \"xi\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_algo = BayesOptSearch(\n",
    "                pbounds,\n",
    "                metric=\"mean_f1_score\",\n",
    "                mode=\"max\",\n",
    "                utility_kwargs=utility_fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # controls the number of trials\n",
    "    \"num_samples\": 100,\n",
    "    \"config\": {\n",
    "        \"xgbclassifier__seed\": 42,\n",
    "    },\n",
    "    \"stop\": {\n",
    "        \"timesteps_total\": 100\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(num_cpus=6, num_gpus=0, include_webui=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "filepath = path.abspath('../datasets/seg_2016_facies/la_team_5_data.h5py')\n",
    "\n",
    "def e2e_seg(config):\n",
    "    return e2e_train_and_test(config, filepath=filepath)\n",
    "\n",
    "def e2e_seg_w_discrete(config):  \n",
    "    return e2e_seg(\n",
    "        parse_config(config)\n",
    "    )\n",
    "\n",
    "analysis = tune.run(e2e_seg_w_discrete,\n",
    "                    name=\"seg_facies_bayes\",\n",
    "                    search_alg=search_algo,\n",
    "                    **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "print(\"Best config: \")\n",
    "pprint(analysis.get_best_config(metric=\"mean_f1_score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.dataframe()\n",
    "top_n_df = df.nlargest(10, \"mean_f1_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_some_tune_results(top_n_df, (0.5, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "from tensorboard import notebook \n",
    "%tensorboard --logdir \"~/ray_results/seg_facies\"\n",
    "notebook.display(height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
